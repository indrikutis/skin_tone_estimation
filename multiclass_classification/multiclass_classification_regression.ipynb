{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Machine Learning - Multiclass classification and regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "from utils import multiclass_classification_utils\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.svm import SVR\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_resuts_folder = \"results/results_v1.12\"\n",
    "\n",
    "if not os.path.exists(save_resuts_folder):\n",
    "    os.makedirs(save_resuts_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data files\n",
    "CHROMA_FIT_data = \"training_data/v2/CHROMA-FIT_data.json\"\n",
    "CHROMA_FIT_CSEC_data = \"training_data/v2/CHROMA-FIT_CSEC_data.json\"\n",
    "CHROMA_FIT_exposure_color_correction_data = (\n",
    "    \"training_data/v2/CHROMA-FIT_exposure_color_correction_data.json\"\n",
    ")\n",
    "\n",
    "MST_data = \"training_data/v2/MST_data.json\"\n",
    "MST_CSEC_data = \"training_data/v2/MST_CSEC_data.json\"\n",
    "MST_exposure_color_correction_data = (\n",
    "    \"training_data/v2/MST_exposure_color_correction_data.json\"\n",
    ")\n",
    "\n",
    "nr_of_test_images = 64\n",
    "\n",
    "# True is used for regression, false for classification models\n",
    "swatches = False\n",
    "\n",
    "all_data = {\n",
    "    # splits\n",
    "    \"CHROMA_FIT_data\": multiclass_classification_utils.prepare_data_with_split(\n",
    "        CHROMA_FIT_data, swatches=swatches\n",
    "    ),\n",
    "    \"CHROMA_FIT_CSEC_data\": multiclass_classification_utils.prepare_data_with_split(\n",
    "        CHROMA_FIT_CSEC_data, swatches=swatches\n",
    "    ),\n",
    "    \"CHROMA_FIT_exposure_color_correction_data\": multiclass_classification_utils.prepare_data_with_split(\n",
    "        CHROMA_FIT_exposure_color_correction_data, swatches=swatches\n",
    "    ),\n",
    "    # original\n",
    "    \"CHROMA_FIT_data_original\": multiclass_classification_utils.prepare_data_with_original(\n",
    "        CHROMA_FIT_data, CHROMA_FIT_data, nr_of_test_images, swatches=swatches\n",
    "    ),\n",
    "    \"CHROMA_FIT_CSEC_data_original\": multiclass_classification_utils.prepare_data_with_original(\n",
    "        CHROMA_FIT_CSEC_data, CHROMA_FIT_data, nr_of_test_images, swatches=swatches\n",
    "    ),\n",
    "    \"CHROMA_FIT_exposure_color_correction_data_original\": multiclass_classification_utils.prepare_data_with_original(\n",
    "        CHROMA_FIT_exposure_color_correction_data,\n",
    "        CHROMA_FIT_data,\n",
    "        nr_of_test_images,\n",
    "        swatches=swatches,\n",
    "    ),\n",
    "    # MST original\n",
    "    \"CHROMA_FIT_data_MST_original\": multiclass_classification_utils.prepare_data_with_MST(\n",
    "        CHROMA_FIT_data, MST_data, swatches=swatches\n",
    "    ),\n",
    "    \"CHROMA_FIT_CSEC_data_MST_original\": multiclass_classification_utils.prepare_data_with_MST(\n",
    "        CHROMA_FIT_CSEC_data, MST_data, swatches=swatches\n",
    "    ),\n",
    "    \"CHROMA_FIT_exposure_color_correction_data_MST_original\": multiclass_classification_utils.prepare_data_with_MST(\n",
    "        CHROMA_FIT_exposure_color_correction_data, MST_data, swatches=swatches\n",
    "    ),\n",
    "    # MST splits\n",
    "    \"CHROMA_FIT_data_MST\": multiclass_classification_utils.prepare_data_with_MST(\n",
    "        CHROMA_FIT_data, MST_data, swatches=swatches\n",
    "    ),\n",
    "    \"CHROMA_FIT_CSEC_data_MST\": multiclass_classification_utils.prepare_data_with_MST(\n",
    "        CHROMA_FIT_CSEC_data, MST_CSEC_data, swatches=swatches\n",
    "    ),\n",
    "    \"CHROMA_FIT_exposure_color_correction_data_MST\": multiclass_classification_utils.prepare_data_with_MST(\n",
    "        CHROMA_FIT_exposure_color_correction_data,\n",
    "        MST_exposure_color_correction_data,\n",
    "        swatches=swatches,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "1. Extracts the pre processed training and testing data\n",
    "2. Runs Random Forest Classifier, logistic Regression, SVC and MLP Classifier models\n",
    "3. The output is saved to the output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in all_data:\n",
    "\n",
    "    print(experiment)\n",
    "    output_file = f\"{save_resuts_folder}/{experiment}.txt\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = all_data[experiment]\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=10, random_state=42, max_depth=5\n",
    "    )  # class_weight='balanced'\n",
    "    multiclass_classification_utils.train_model_classification(\n",
    "        rf, experiment, output_file, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    lr = LogisticRegression(random_state=42)  #  class_weight='balanced'\n",
    "    multiclass_classification_utils.train_model_classification(\n",
    "        lr, experiment, output_file, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    svm = SVC(kernel=\"rbf\", random_state=42)  # class_weight='balanced'\n",
    "    multiclass_classification_utils.train_model_classification(\n",
    "        svm, experiment, output_file, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "    multiclass_classification_utils.train_model_classification(\n",
    "        mlp, experiment, output_file, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass regression\n",
    "\n",
    "1. Extract the pre processed training and testing data\n",
    "2. Runs XGBoost, Multi-Layer Perceptron Regressor, Support Vector Regression, LightGBM\n",
    "3. The output is saved to the output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in all_data:\n",
    "\n",
    "    print(experiment)\n",
    "    output_file = f\"{save_resuts_folder}/{experiment}.txt\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = all_data[experiment]\n",
    "\n",
    "    xg_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100, random_state=42, objective=\"reg:squarederror\"\n",
    "    )\n",
    "    multi_output_xgb = MultiOutputRegressor(xg_reg)\n",
    "    multiclass_classification_utils.train_model_regression(\n",
    "        multi_output_xgb,\n",
    "        experiment,\n",
    "        output_file,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        swatches=True,\n",
    "    )\n",
    "\n",
    "    mlp_regressor = MLPRegressor(\n",
    "        hidden_layer_sizes=(100, 100), max_iter=300, random_state=42\n",
    "    )\n",
    "    multiclass_classification_utils.train_model_regression(\n",
    "        mlp_regressor,\n",
    "        experiment,\n",
    "        output_file,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        swatches=True,\n",
    "    )\n",
    "\n",
    "    # param_grid = {\n",
    "    #     'estimator__C': [1, 10, 100, 1000],  # Regularization parameter\n",
    "    #     'estimator__epsilon': [0.01, 0.1, 0.2],  # Epsilon parameter (margin of error)\n",
    "    #     'estimator__kernel': ['rbf', 'linear', 'poly'],  # Types of kernel\n",
    "    #     'estimator__gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf' and 'poly'\n",
    "    # }\n",
    "\n",
    "    # # Initialize the SVR and MultiOutputRegressor\n",
    "    # svr = SVR()\n",
    "    # multi_output_svr = MultiOutputRegressor(svr)\n",
    "\n",
    "    # # Create the GridSearchCV object\n",
    "    # grid_search = GridSearchCV(\n",
    "    #     multi_output_svr,\n",
    "    #     param_grid,\n",
    "    #     cv=5,  # 5-fold cross-validation\n",
    "    #     scoring='neg_mean_squared_error',  # Use MSE for scoring\n",
    "    #     verbose=1,\n",
    "    #     n_jobs=-1  # Use all available CPU cores\n",
    "    # )\n",
    "\n",
    "    # multiclass_classification_utils.train_model_regression(grid_search, experiment, output_file, X_train, X_test, y_train, y_test, swatches=True)\n",
    "    # Best parameters: {'estimator__C': 100, 'estimator__epsilon': 0.1, 'estimator__gamma': 'scale', 'estimator__kernel': 'rbf'}\n",
    "\n",
    "    svr = SVR(kernel=\"rbf\")\n",
    "    # svr = SVR(kernel='rbf', C=100, epsilon=0.1, gamma='scale')\n",
    "    multi_output_svr = MultiOutputRegressor(svr)\n",
    "    multiclass_classification_utils.train_model_regression(\n",
    "        multi_output_svr,\n",
    "        experiment,\n",
    "        output_file,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        swatches=True,\n",
    "    )\n",
    "\n",
    "    lgb_regressor = lgb.LGBMRegressor(n_estimators=100, random_state=42)\n",
    "    multi_output_lgb = MultiOutputRegressor(lgb_regressor)\n",
    "    multiclass_classification_utils.train_model_regression(\n",
    "        multi_output_lgb,\n",
    "        experiment,\n",
    "        output_file,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        swatches=True,\n",
    "    )\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the SVR model performed the best - fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"estimator__C\": np.logspace(-3, 3, 7),\n",
    "    \"estimator__gamma\": [\"scale\", \"auto\", 0.1, 0.01, 1],\n",
    "    \"estimator__epsilon\": [0.01, 0.1, 0.2, 0.5],\n",
    "    \"estimator__kernel\": [\"rbf\", \"linear\", \"poly\"],\n",
    "    # 'estimator__tol': [1e-4, 1e-3, 1e-2],  # Tolerance for stopping criteria\n",
    "}\n",
    "\n",
    "for experiment in all_data:\n",
    "\n",
    "    output_file = f\"{save_resuts_folder}/{experiment}.txt\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = all_data[experiment]\n",
    "\n",
    "    svr = SVR(kernel=\"rbf\")\n",
    "    multi_output_svr = MultiOutputRegressor(svr)\n",
    "    print(\"Default Parameters of multi_output_svr:\", multi_output_svr.get_params())\n",
    "    grid_search = GridSearchCV(multi_output_svr, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "    multiclass_classification_utils.train_model_regression(\n",
    "        grid_search,\n",
    "        experiment,\n",
    "        output_file,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        swatches=True,\n",
    "    )\n",
    "\n",
    "    # Fine tune on one experiment\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis for classification model\n",
    "\n",
    "1. Identify patterns or trends in high-dimensional data\n",
    "2. Simplify data for visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use swatches to display data points as MST colors\n",
    "\n",
    "SWATCHES_MAPPING = {\n",
    "    1: [246, 237, 228],\n",
    "    2: [243, 231, 219],\n",
    "    3: [247, 234, 208],\n",
    "    4: [234, 218, 186],\n",
    "    5: [215, 189, 150],\n",
    "    6: [160, 126, 86],\n",
    "    7: [130, 92, 67],\n",
    "    8: [96, 65, 52],\n",
    "    9: [58, 49, 42],\n",
    "    10: [41, 36, 32],\n",
    "}\n",
    "\n",
    "SWATCHES_MAPPING_NORMALIZED = {\n",
    "    k: np.array(v) / 255.0 for k, v in SWATCHES_MAPPING.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA data\n",
    "\n",
    "swatches = False\n",
    "\n",
    "all_data = {\n",
    "    # splits\n",
    "    \"CHROMA_FIT_data\": multiclass_classification_utils.prepare_data_with_split(\n",
    "        CHROMA_FIT_data, swatches=swatches\n",
    "    ),\n",
    "    \"CHROMA_FIT_CSEC_data\": multiclass_classification_utils.prepare_data_with_split(\n",
    "        CHROMA_FIT_CSEC_data, swatches=swatches\n",
    "    ),\n",
    "    \"CHROMA_FIT_exposure_color_correction_data\": multiclass_classification_utils.prepare_data_with_split(\n",
    "        CHROMA_FIT_exposure_color_correction_data, swatches=swatches\n",
    "    ),\n",
    "    # MST splits\n",
    "    \"MST_data\": multiclass_classification_utils.prepare_data_with_split(\n",
    "        MST_data, swatches=swatches\n",
    "    ),\n",
    "    \"MST_CSEC_data\": multiclass_classification_utils.prepare_data_with_split(\n",
    "        MST_CSEC_data, swatches=swatches\n",
    "    ),\n",
    "    \"MST_exposure_color_correction_data\": multiclass_classification_utils.prepare_data_with_split(\n",
    "        MST_exposure_color_correction_data, swatches=swatches\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder to save the PCA plots\n",
    "save_folder = \"/home/dasec-notebook/Thesis/visualization/PCA/MST/classification\"\n",
    "\n",
    "for experiment in all_data:\n",
    "\n",
    "    file_name = f\"{save_folder}/{experiment}.png\"\n",
    "    print(experiment)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = all_data[experiment]\n",
    "\n",
    "    # concatenate the training and test data to display all data\n",
    "    X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "    # Fit PCA\n",
    "    pca = PCA().fit(X_train)\n",
    "    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Plot cumulative explained variance\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(\n",
    "        range(1, len(explained_variance) + 1),\n",
    "        explained_variance,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Cumulative Explained Variance\")\n",
    "    # plt.title('Explained Variance vs. Number of Components')\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.xticks(np.arange(1, len(explained_variance) + 1, 1))\n",
    "    plt.axhline(y=0.95, color=\"r\", linestyle=\"--\", label=\"95% Variance Explained\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{save_folder}/{experiment}_variance.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Find the number of components that explain 95% of variance and plot PCA\n",
    "    n_components = np.argmax(explained_variance >= 0.95) + 1\n",
    "    print(f\"Number of components for 95% variance explained: {n_components}\")\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "    if swatches:\n",
    "        y_train = y_train / 255.0\n",
    "\n",
    "    colors = np.array([SWATCHES_MAPPING_NORMALIZED[y] for y in y_train])\n",
    "\n",
    "    plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], color=colors)\n",
    "    # plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], color='blue')\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.savefig(f\"{save_folder}/{experiment}_PCA.png\")\n",
    "    plt.title(\"PCA of Training Data\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the data with confidence ellipses\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    ax = plt.gca()\n",
    "    for class_label in np.unique(y_train):\n",
    "        class_mask = y_train == class_label\n",
    "        class_points = X_train_pca[class_mask]\n",
    "        multiclass_classification_utils.plot_confidence_ellipse(\n",
    "            ax,\n",
    "            class_points,\n",
    "            color=SWATCHES_MAPPING_NORMALIZED[class_label],\n",
    "            label=f\"Class {class_label}\",\n",
    "        )\n",
    "        plt.scatter(\n",
    "            class_points[:, 0],\n",
    "            class_points[:, 1],\n",
    "            color=SWATCHES_MAPPING_NORMALIZED[class_label],\n",
    "            s=10,\n",
    "            label=f\"Points {class_label}\",\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{save_folder}/{experiment}_PCA_with_ellipses.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve for multiclass classification\n",
    "\n",
    "1. Evaluate the performance of a model\n",
    "2. A higher AUC indicates a better ability to distinguish the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in all_data:\n",
    "\n",
    "    print(experiment)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = all_data[experiment]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=10, random_state=42, max_depth=5)\n",
    "    multiclass_classification_utils.plot_multiclass_roc_curve(\n",
    "        rf, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    lr = LogisticRegression(random_state=42)\n",
    "    multiclass_classification_utils.plot_multiclass_roc_curve(\n",
    "        lr, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    svm = SVC(kernel=\"rbf\", random_state=42)\n",
    "    multiclass_classification_utils.plot_multiclass_roc_curve(\n",
    "        svm, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "    multiclass_classification_utils.plot_multiclass_roc_curve(\n",
    "        mlp, X_train, X_test, y_train, y_test\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
