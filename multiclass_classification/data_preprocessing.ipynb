{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIQA underexposure and OFIQ illumination uniformity feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import csv\n",
    "\n",
    "sys.path.append('/home/dasec-notebook/Thesis/OFIQ-Project/python')\n",
    "from ofiq_zmq import OfiqZmq\n",
    "\n",
    "from utils import multiclass_classification_utils \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"/home/dasec-notebook/Thesis/Datasets/mst-e_data/CSEC\"\n",
    "OUTPUT_FOLDER = \"/home/dasec-notebook/Thesis/Datasets/mst-e_data/FIQA_CSEC\"\n",
    "shape_predictor_path = \"/home/dasec-notebook/Thesis/skin_tone_estimation/shape_predictor_68_face_landmarks.dat\"  # Download this file\n",
    "OUTPUT_SIZE = 2048\n",
    "TRANSFORM_SIZE = 4096\n",
    "ENABLE_PADDING = True\n",
    "\n",
    "# Process images\n",
    "multiclass_classification_utils.process_images(\n",
    "    INPUT_FOLDER,\n",
    "    OUTPUT_FOLDER,\n",
    "    OUTPUT_SIZE,\n",
    "    shape_predictor_path,\n",
    "    TRANSFORM_SIZE,\n",
    "    ENABLE_PADDING,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append a full path to the image\n",
    "\n",
    "Needed because there are some images which have duplicate names and during the extraction their file names have been changed to include _1/_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directories\n",
    "image_dir = \"/home/dasec-notebook/Thesis/Datasets/CHROMA-FIT-Dataset/DATA_CROPPED_PORTRAIT2_illuminant_correction/exposure_color_correction\"\n",
    "input_csv = \"exposure_features/original/CHROMA-FIT_exposure_color_correction_predictions_predictions.csv\"\n",
    "output_csv = \"exposure_features/original/CHROMA-FIT_exposure_color_correction_predictions_predictions.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "full_paths = []\n",
    "\n",
    "# Recursively map images to their folder paths\n",
    "for _, row in df.iterrows():\n",
    "    image_name = row[\"Image\"]\n",
    "    image_found = False\n",
    "    for root, dirs, files in os.walk(image_dir):\n",
    "        if image_name in files:\n",
    "            relative_path = os.path.relpath(root, image_dir)\n",
    "            full_paths.append(f\"{relative_path}/{image_name}\")\n",
    "            image_found = True\n",
    "            break\n",
    "    if not image_found:\n",
    "        print(image_name)\n",
    "        full_paths.append(\"NOT_FOUND\")\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "df[\"Image\"] = full_paths\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Updated CSV file has been saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "image_root = \"/home/dasec-notebook/Thesis/Datasets/mst-e_data/mst-e_data_portrait\"\n",
    "csv_file = \"/home/dasec-notebook/Thesis/skin_tone_estimation/multiclass_classification/exposure_features/formatted/MST_predictions.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Define the intervals\n",
    "# intervals = [(i / 10, (i + 2) / 10) for i in range(0, 10, 2)]  # [(0.0, 0.2), (0.2, 0.4), ..., (0.8, 1.0)]\n",
    "intervals = [\n",
    "    (i / 4, (i + 1) / 4) for i in range(4)\n",
    "]  # [(0.0, 0.25), (0.25, 0.5), (0.5, 0.75), (0.75, 1.0)]\n",
    "\n",
    "\n",
    "# Loop through each interval\n",
    "for idx, (low, high) in enumerate(intervals):\n",
    "\n",
    "    filtered = df[(df[\"Prob_Class_0\"] >= low) & (df[\"Prob_Class_0\"] < high)]\n",
    "    sample = filtered.sample(min(3, len(filtered)))  # Take up to 2 samples\n",
    "\n",
    "    print(f\"Interval {low}-{high}\")\n",
    "    fig, axes = plt.subplots(1, len(sample), figsize=(10, 6))\n",
    "\n",
    "    # Add images to subplots\n",
    "    for j, (_, row) in enumerate(sample.iterrows()):\n",
    "        image_path = os.path.join(image_root, row[\"Image\"])\n",
    "        img = Image.open(image_path)\n",
    "        axes[j].imshow(img)\n",
    "        axes[j].set_title(f\"Prob_Class_0: {row['Prob_Class_0']:.2f}\")\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OFIQ illuminanation uniformity feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ofiq_zmq = OfiqZmq(\"/home/dasec-notebook/Thesis/OFIQ-Project\")\n",
    "\n",
    "dataset_path = \"//home/dasec-notebook/Thesis/Datasets/mst-e_data/mst-e_data_portrait_illuminant_correction/exposure_color_correction\"\n",
    "images_path = \"../image_paths/MST.txt\"\n",
    "save_path = \"exposure_features/OFIQ/MST_exposure_color_correction.csv\"\n",
    "\n",
    "exposure_df = pd.DataFrame(columns=[\"image_path\", \"illumination_uniformity\"])\n",
    "\n",
    "# load image paths\n",
    "with open(images_path, \"r\") as f:\n",
    "    image_paths = f.readlines()\n",
    "\n",
    "for image_path in image_paths:\n",
    "\n",
    "    try:\n",
    "        image = os.path.join(dataset_path, image_path.strip())\n",
    "        result = ofiq_zmq.process_image(image)\n",
    "        illumination_uniformity = result[\"quality_assessments\"][67].raw_score\n",
    "\n",
    "        # Append the results to the DataFrame using pd.concat instead of append\n",
    "        exposure_df = pd.concat(\n",
    "            [\n",
    "                exposure_df,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"image_path\": [image_path.strip()],\n",
    "                        \"illumination_uniformity\": [illumination_uniformity],\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {image_path}\")\n",
    "\n",
    "exposure_df.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append OFIQ exposure to training data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON saved to training_data/v2/MST_data.json\n"
     ]
    }
   ],
   "source": [
    "# Paths to input files\n",
    "json_file_path = \"training_data/v1/MST_data.json\"\n",
    "csv_file_path = \"exposure_features/OFIQ/MST.csv\"\n",
    "output_json_path = \"training_data/v2/MST_data.json\"\n",
    "\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Load the CSV data into a dictionary\n",
    "illumination_data = {}\n",
    "with open(csv_file_path, \"r\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        dir_and_file = row[\"image_path\"]\n",
    "        illumination_value = float(row[\"illumination_uniformity\"])\n",
    "        illumination_data[dir_and_file] = illumination_value\n",
    "\n",
    "# Add illumination_uniformity to the JSON data\n",
    "for dir_id, dir_data in data.items():\n",
    "    for image_id in dir_data.keys():\n",
    "        if image_id == \"MST_value\":\n",
    "            continue\n",
    "\n",
    "        csv_key = f\"{dir_id}/{image_id}\"\n",
    "        if csv_key in illumination_data.keys():\n",
    "            dir_data[image_id][\"OFIQ_illumination_uniformity\"] = illumination_data[\n",
    "                csv_key\n",
    "            ]\n",
    "\n",
    "# Save the updated JSON data to a new file\n",
    "with open(output_json_path, \"w\") as output_file:\n",
    "    json.dump(data, output_file, indent=4)\n",
    "\n",
    "print(f\"Updated JSON saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all class values in one json file to be used in multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths to your input files\n",
    "# cheek_rgb_path = \"../baseline/results/cheek_colors_extracted/MST_exposure_color_correction.json\"\n",
    "# sclera_rgb_path = \"../sclera_segmentation/results/MST_exposure_color_correction_sclera_RGB.json\"\n",
    "# exposure_csv_path = \"exposure_features/formatted/MST_exposure_color_correction_predictions.csv\"\n",
    "\n",
    "# combined_output_path = \"training_data/MST_exposure_color_correction_data.json\"\n",
    "\n",
    "cheek_rgb_path = \"../baseline/results/cheek_colors_extracted/CHROMA-FIT_exposure_color_correction_outdoor.json\"\n",
    "sclera_rgb_path = \"../sclera_segmentation/results/CHROMA-FIT_exposure_color_correction_sclera_RGB.json\"\n",
    "exposure_csv_path = (\n",
    "    \"exposure_features/formatted/CHROMA-FIT_exposure_color_correction_predictions.csv\"\n",
    ")\n",
    "ground_truth_path = \"../image_paths/ground_truth_CHROMA-FIT_forehead.csv\"\n",
    "\n",
    "combined_output_path = (\n",
    "    \"training_data/CHROMA-FIT_exposure_color_correction_outdoor_data.json\"\n",
    ")\n",
    "\n",
    "# NOTE: for MST dataset only\n",
    "MST_SUBJECT_MAPPING = {\n",
    "    \"subject_18\": 1,\n",
    "    \"subject_17\": 8,\n",
    "    \"subject_16\": 1,\n",
    "    \"subject_15\": 3,\n",
    "    \"subject_14\": 6,\n",
    "    \"subject_13\": 2,\n",
    "    \"subject_12\": 10,\n",
    "    \"subject_11\": 5,\n",
    "    \"subject_10\": 9,\n",
    "    \"subject_9\": 4,\n",
    "    \"subject_8\": 2,\n",
    "    \"subject_7\": 4,\n",
    "    \"subject_6\": 5,\n",
    "    \"subject_5\": 7,\n",
    "    \"subject_4\": 9,\n",
    "    \"subject_3\": 6,\n",
    "    \"subject_2\": 8,\n",
    "    \"subject_1\": 2,\n",
    "    \"subject_0\": 3,\n",
    "}\n",
    "\n",
    "\n",
    "# NOTE: just for CHROMA-FIT dataset\n",
    "ground_truth_avg_data = {}\n",
    "with open(ground_truth_path, \"r\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        subject_id = f\"{int(row['ID']):05d}\"  # Convert to five-digit format\n",
    "        ground_truth_avg = int(row[\"Ground_Truth_Avg\"])\n",
    "        ground_truth_avg_data[subject_id] = ground_truth_avg\n",
    "\n",
    "# Load JSON data\n",
    "with open(sclera_rgb_path, \"r\") as sclera_file:\n",
    "    sclera_data = json.load(sclera_file)\n",
    "\n",
    "with open(cheek_rgb_path, \"r\") as csec_file:\n",
    "    cheek_rgb = json.load(csec_file)\n",
    "\n",
    "# Load CSV data\n",
    "FIQA_exposure = {}\n",
    "with open(exposure_csv_path, \"r\") as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        # Extract relevant columns\n",
    "        image = row[\"Image\"].split(\"/\")[-1]  # Get the file name only\n",
    "        prob_class_0 = float(row[\"Prob_Class_0\"])\n",
    "        FIQA_exposure[image] = {\"Prob_Class_0\": prob_class_0}\n",
    "\n",
    "combined_results = {}\n",
    "\n",
    "for subject, images in cheek_rgb.items():\n",
    "    combined_results[subject] = {}\n",
    "    # mst_value = MST_SUBJECT_MAPPING.get(subject, None)  # NOTE: For MST dataset only\n",
    "    mst_value = ground_truth_avg_data.get(\n",
    "        subject, None\n",
    "    )  # NOTE: For CHROMA-FIT dataset only\n",
    "    combined_results[subject][\"MST_value\"] = mst_value\n",
    "\n",
    "    for image, csec_values in images.items():\n",
    "        combined_results[subject][image] = {\"CSEC_cheek_colors\": csec_values}\n",
    "\n",
    "        # Add CSEC cheek colors\n",
    "        if subject in sclera_data and image in sclera_data[subject]:\n",
    "            combined_results[subject][image][\"sclera_RGB\"] = sclera_data[subject][image]\n",
    "\n",
    "        # Add CHROMA-FIT predictions\n",
    "        if image in FIQA_exposure:\n",
    "            combined_results[subject][image][\"FIQA_exposure\"] = FIQA_exposure[image]\n",
    "\n",
    "# Save the combined results to a JSON file\n",
    "with open(combined_output_path, \"w\") as output_file:\n",
    "    json.dump(combined_results, output_file, indent=4)\n",
    "\n",
    "print(f\"Combined results saved to {combined_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
